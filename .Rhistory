num = flipper_length_mm, denom = bill_length_mm
) |>
select(ratio)
View(.Last.value)
calc_ratio_w_name <- function(df, num, denom) {
name <- glue::glue(
'ratio_{ensym(num)}_{ensym(denom)}'
##ensym() comes from the rlang package in R (part of the tidyverse ecosystem). It‚Äôs used in tidy      ##evaluation to capture an argument as a symbol (not evaluated immediately).
)
df |>
mutate(
{{name}} := {{num}} / {{denom}} ## uses := to asign the names to the operation
)
}
palmerpenguins::penguins |>
calc_ratio_w_name(
num = flipper_length_mm, denom = bill_length_mm
)
View(.Last.value)
palmerpenguins::penguins |>
calc_ratio_w_name(
num = flipper_length_mm, denom = bill_length_mm
) |>
calc_ratio_w_name(
num = body_mass_g, denom = bill_length_mm
) |>
select(matches('ratio'))
View(.Last.value)
library(readr)
library(tidyverse)
library(openxlsx)
library(here)
library(tidyxl)
library(tidyr)
library(dplyr)
library(glue)
library(epoxy)
library(rprojroot)
library(purrr)
call_NWS <- function(coordinates = '38.8894,-77.0352') {
NWS_base_url <- 'https://api.weather.gov'
httr2::request(NWS_base_url)  |>
httr2::req_url_path_append(
'points',
coordinates
) |>
httr2::req_perform() |>
httr2::resp_body_json()
}
View(.Last.value)
View(call_NWS)
View(call_NWS)
get_NWS_forecast <- function(forecast_url) {
httr2::request(forecast_url)  |>
httr2::req_perform() |>
httr2::resp_body_json()
}
View(get_NWS_forecast)
forecast_url <- call_NWS() |>
pluck('properties', 'forecastHourly')
get_NWS_forecast(forecast_url) |>
pluck('properties', 'periods', 1) |>
glimpse()
get_NWS_forecast(forecast_url) |>
pluck('properties', 'periods') |>
bind_rows() |>
select(
time = startTime,
temp_f = temperature,
rain_prob = probabilityOfPrecipitation,
forecast = shortForecast
) |>
filter(
map_lgl(rain_prob, is.numeric)
) |>
mutate(
rain_prob = map_dbl(rain_prob, 1),
time = ymd_hms(time)
)
get_NWS_forecast(forecast_url) |>
pluck('properties', 'periods') |>
map_dfr(
\(list_of_16) {
tibble(
time = list_of_16 |> pluck('startTime'),
temp_F = list_of_16 |> pluck('temperature'),
rain_prob = list_of_16 |>
pluck('probabilityOfPrecipitation', 'value'),
forecast = list_of_16 |> pluck('shortForecast')
)
}
) |>
mutate(time = ymd_hms(time))
get_weather_forecast <- function(coordinates = '38.8894,-77.0352') {
forecast_url <- call_NWS(coordinates) |>
pluck('properties', 'forecastHourly')
get_NWS_forecast(forecast_url) |>
pluck('properties', 'periods') |>
map_dfr(
\(list_of_16) {
tibble(
time = list_of_16 |> pluck('startTime'),
temp_F = list_of_16 |> pluck('temperature'),
rain_prob = list_of_16 |>
pluck('probabilityOfPrecipitation', 'value'),
forecast = list_of_16 |> pluck('shortForecast')
)
}
) |>
mutate(time = ymd_hms(time))
}
get_weather_forecast()
get_weather_forecast('sgsdfsd')
call_NWS_with_backoff <- insistently(call_NWS)
tryCatch({
call_NWS_with_backoff('sgsdfsd') |>
pluck('properties', 'forecastHourly')
}, error = function(e) {
cli::cli_abort(
"Operation failed with this error message: {.val {e$message}}.
This likely happened because the {.arg coordinates} argument isn't valid."
)
})
get_weather_forecast_with_backoff <- function(
coordinates = '38.8894,-77.0352'
) {
forecast_url <- tryCatch({
call_NWS_with_backoff(coordinates) |>
pluck('properties', 'forecastHourly')
}, error = function(e) {
cli::cli_abort(
"Operation failed with this error message: {.val {e$message}}.
This likely happened because the {.arg coordinates} argument isn't valid."
)
})
get_NWS_forecast(forecast_url) |>
pluck('properties', 'periods') |>
map_dfr(
\(list_of_16) {
tibble(
time = list_of_16 |> pluck('startTime'),
temp_F = list_of_16 |> pluck('temperature'),
rain_prob = list_of_16 |>
pluck('probabilityOfPrecipitation', 'value'),
forecast = list_of_16 |> pluck('shortForecast')
)
}
) |>
mutate(time = ymd_hms(time))
}
get_weather_forecast_with_backoff('adfsdf')
get_weather_forecast_with_backoff()
## management of periods and time operations
days(10) + hours(1)
tib <- tibble(
periods = c(
days(10),
hours(2),
days(3),
hours(6)
)
)
sum(tib$periods)
reduce(tib$periods, `+`)
tmp <- days(0)
for (current_period in tib$periods) {
tmp <- tmp + current_period
}
tmp
tmp <- days(0)
for (current_period in list(days(10), hours(2))) {
tmp <- tmp + current_period
}
tmp
tmp <- days(0)
for (current_period in map(tib$periods, as.period)) {
tmp <- tmp + current_period
}
tmp
setwd("~/Personal_RM/Libro/introduccion_cap1")
setwd("~/Personal_RM/Libro/capitulo_2")
system("quarto pandoc capitulo_2.docx -t markdown -o capitulo_2.qmd")
setwd("~/Personal_RM/Libro/capitulo_3")
system("quarto pandoc capitulo_3.docx -t markdown -o capitulo_3.qmd")
setwd("~/Personal_RM/Libro/capitulo_4")
system("quarto pandoc capitulo_4.docx -t markdown -o capitulo_4.qmd")
setwd("~/Personal_RM/Libro/capitulo_5")
system("quarto pandoc capitulo_5.docx -t markdown -o capitulo_5.qmd")
setwd("~/Personal_RM/Libro/capitulo_6")
system("quarto pandoc capitulo_6.docx -t markdown -o capitulo_6.qmd")
setwd("~/Personal_RM/Libro/capitulo_7_epilogo")
system("quarto pandoc capitulo_7.docx -t markdown -o capitulo_7.qmd")
setwd("~/Personal_RM/Libro")
quarto render
quarto render
setwd("~/Personal_RM/Libro")
quarto::quarto_path()
install.packages("quarto")
quarto::quarto_path()
getwd()
quarto::quarto_render()
setwd("~/Personal_RM/Libro")
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
setwd("~/Personal_RM/Libro")
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
## word to quarto
"LIC" %in% installed.packages()
rmarkdown::find_pandoc()
setwd("~/Personal_RM/Libro/word_quarto")
system("quarto pandoc capitulo 1.docx -t markdown -o capitulo 1.qmd")
install.packages("quarto")
quarto::quarto_render()
## word to quarto
"LIC" %in% installed.packages()
### quarto render
quarto::quarto_path()
getwd()
rmarkdown::find_pandoc()
system("quarto pandoc capitulo 1.docx -t markdown -o capitulo 1.qmd")
setwd("~/Personal_RM/Libro/word_quarto")
system("quarto pandoc capitulo_1.docx -t markdown -o capitulo_1.qmd")
system("quarto pandoc capitulo_1.docx -t markdown -o capitulo_1.qmd")
system("quarto pandoc capitulo_1.docx -t markdown -o capitulo_1.qmd")
system("quarto pandoc introduccion.docx -t markdown -o introduccion.qmd")
quarto::quarto_render()
setwd("~/Personal_RM/Libro")
quarto::quarto_render()
quarto::quarto_render()
setwd("~/Personal_RM/Libro")
quarto::quarto_render()
setwd("~/Personal_RM/Libro/_book")
quarto::quarto_render()
setwd("~/Personal_RM/Libro")
quarto::quarto_render()
setwd("~/Escritorio/Libro")
system("quarto pandoc capitulo_1.docx -t markdown -o capitulo_1.qmd")
setwd("~/Personal_RM/Libro")
quarto::quarto_render()
setwd("~/Personal_RM/Libro")
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
setwd("~/Personal_RM/Libro")
quarto::quarto_render()
quarto::quarto_render()
quarto preview
quarto preview
setwd("~/Personal_RM/Libro/_book")
setwd("~/Personal_RM/Libro/Doc_Links")
library(officer)
library(stringr)
library(httr)
library(dplyr)
library(purrr)
library(xml2)
library(stringr)
# Ruta del archivo Word
archivo_word <- "/home/ricardo/Personal_RM/Libro/Doc_Links/El_amor_se_paga_con_amor_agosto_9_2025.docx"
file.exists(archivo_word)
# Crea una carpeta temporal
carpeta_temp <- "temp_docx"
unzip(archivo_word, exdir = carpeta_temp)
## Leer todos los footnotes y contarlos
footnotes <- read_xml(file.path(carpeta_temp, "word/footnotes.xml"))
texto_footnotes <- xml_text(footnotes)
todos_los_footnotes<- xml_find_all(footnotes, "//w:footnote")
length(todos_los_footnotes)
# Busca URLs dentro de las notas al pie
urls_footnotes <- str_extract_all(texto_footnotes, "(https?://|www\\.)[^ ]+")[[1]]
urls_footnotes <- unique(urls_footnotes)
## cantidad encontrada de urls
length(urls_footnotes)
head(urls_footnotes)
write.csv(todos_los_footnotes, "todas_las_urls.csv", row.names = FALSE)
# Ruta del archivo Word
archivo_word <- "/home/ricardo/Personal_RM/Libro/Doc_Links/El_amor_se_paga_con_amor_agosto_9_2025.docx"
# Crea una carpeta temporal
carpeta_temp <- "temp_docx"
unzip(archivo_word, exdir = carpeta_temp)
## Leer todos los footnotes y contarlos
footnotes <- read_xml(file.path(carpeta_temp, "word/footnotes.xml"))
texto_footnotes <- xml_text(footnotes)
footnotes<- xml_find_all(footnotes, "//w:footnote")
length(footnotes)
footnotes_texto_individual <- xml_find_all(footnotes, "//w:footnote") %>%
map_chr(xml_text)
# Crear un archivo de salida
archivo_salida <- "/home/ricardo/Personal_RM/Libro/Doc_Links/footnotes_completos.txt"
# Escribir todos los footnotes en el archivo
writeLines(footnotes_texto_individual, archivo_salida)
# Confirmar
cat("Se guardaron", length(footnotes_texto_individual), "footnotes en:", archivo_salida)
# Ruta del archivo Word
archivo_word <- "/home/ricardo/Personal_RM/Libro/Doc_Links/El_amor_se_paga_con_amor_agosto_9_2025.docx"
# Crea una carpeta temporal
carpeta_temp <- "temp_docx"
unzip(archivo_word, exdir = carpeta_temp)
## Leer todos los footnotes y contarlos
footnotes <- read_xml(file.path(carpeta_temp, "word/footnotes.xml"))
footnotes_texto_individual <- xml_find_all(footnotes, "//w:footnote") %>%
map_chr(xml_text)
footnotes_texto_individual <- xml_find_all(footnotes, "//w:footnote") %>%
map_chr(xml_text)
df_footnotes <- tibble(
id = seq_along(footnotes_texto_individual),
texto = footnotes_texto_individual
)
View(df_footnotes)
# Ruta del archivo Word
archivo_word <- "/home/ricardo/Personal_RM/Libro/Doc_Links/El_amor_se_paga_con_amor_agosto_9_2025.docx"
# Crea una carpeta temporal
carpeta_temp <- "temp_docx"
unzip(archivo_word, exdir = carpeta_temp)
## Leer todos los footnotes y contarlos
footnotes <- read_xml(file.path(carpeta_temp, "word/footnotes.xml"))
unzip(archivo_word, exdir = carpeta_temp)
## Leer todos los footnotes y contarlos
footnotes <- read_xml(file.path(carpeta_temp, "word/footnotes.xml"))
footnotes_texto_individual <- xml_find_all(footnotes, "//w:footnote") %>%
map_chr(xml_text)
df_footnotes <- tibble(
id = seq_along(footnotes_texto_individual),
texto = footnotes_texto_individual
)
# Raz√≥n por la que salen dos espacios en blanco al inicio del df_footnotes
# the first two <w:footnote> nodes inside footnotes.xml are special footnotes used by Word, not actual ones from your document.
# They usually have w:id="-1" and w:id="0" and are reserved for things like separator lines or continuation separators.
# Guardar como CSV
archivo_footnotes <- "/home/ricardo/Personal_RM/Libro/Doc_Links/footnotes_completos.csv"
write.csv(df_footnotes, archivo_footnotes, row.names = FALSE)
df_footnotes_urls <- df_footnotes %>%
mutate(urls = str_extract_all(texto, "https?://[^\\s]+"))
# Guardar en otro archivo
archivo_footnotes_urls <- "/home/ricardo/Personal_RM/Libro/Doc_Links/footnotes_con_id_urls.csv"
write.csv(df_footnotes_urls, archivo_footnotes_urls, row.names = FALSE)
df_footnotes_urls <- df_footnotes %>%
mutate(urls = str_extract_all(texto, "https?://[^\\s]+")) %>%
mutate(urls = map_chr(urls, ~ if (length(.x) == 0) "" else paste(.x, collapse = ", ")))
df_footnotes_urls <- df_footnotes %>%
mutate(urls = str_extract_all(texto, "https?://[^\\s]+")) %>%
mutate(urls = map_chr(urls, ~ if (length(.x) == 0) "" else paste(.x, collapse = ", ")))
# Guardar como CSV
archivo_footnotes_urls <- "/home/ricardo/Personal_RM/Libro/Doc_Links/footnotes_con_id_urls.csv"
write.csv(df_footnotes_urls, archivo_footnotes_urls, row.names = FALSE)
cat("‚úÖ Archivo con footnotes + URLs guardado en:\n", archivo_footnotes_urls)
df_urls_expandido <- df_footnotes_urls %>%
# Filtrar solo los footnotes que tienen URLs
filter(urls != "") %>%
# Separar las URLs en filas individuales
separate_rows(urls, sep = ",\\s*") %>%
# Limpiar espacios extra
mutate(urls = str_trim(urls))
library(tidyr)
df_urls_expandido <- df_footnotes_urls %>%
# Filtrar solo los footnotes que tienen URLs
filter(urls != "") %>%
# Separar las URLs en filas individuales
separate_rows(urls, sep = ",\\s*") %>% ## funci√≥n de tidyr
# Limpiar espacios extra
mutate(urls = str_trim(urls))
# Guardar este nuevo archivo
archivo_urls_expandido <- "/home/ricardo/Personal_RM/Libro/Doc_Links/footnotes_urls_expandido.csv"
df_urls_expandido <- read_csv(archivo_urls_expandido)
library(readr)
df_urls_expandido <- read_csv(archivo_urls_expandido)
# Guardar este nuevo archivo
archivo_urls_expandido <- "/home/ricardo/Personal_RM/Libro/Doc_Links/footnotes_urls_expandido.csv"
df_urls_expandido <- read_csv(archivo_urls_expandido)
View(df_urls_expandido)
df_urls_expandido <- df_footnotes_urls %>%
# Filtrar solo los footnotes que tienen URLs
filter(urls != "") %>%
# Separar las URLs en filas individuales
separate_rows(urls, sep = ",\\s*") %>% ## funci√≥n de tidyr
# Limpiar espacios extra
mutate(urls = str_trim(urls))
2Ô∏è‚É£ Guardar el archivo expandido (opcional)
archivo_urls_expandido <- "/home/ricardo/Personal_RM/Libro/Doc_Links/footnotes_urls_expandido.csv"
write.csv(df_urls_expandido, archivo_urls_expandido, row.names = FALSE)
check_url_status <- function(url) {
tryCatch({
resp <- httr::HEAD(url, timeout(10)) # usamos HEAD porque es m√°s r√°pido que GET
status <- httr::status_code(resp)
return(status)
}, error = function(e) {
return(NA) # si hay error (timeout, DNS, etc.)
})
}
df_urls_verificadas <- df_urls_expandido %>%
mutate(status_code = map_int(urls, check_url_status))
df_errores_urls <- df_urls_verificadas %>%
filter(is.na(status_code) | status_code != 200)
# üíæ Guardar en nuevo archivo CSV
archivo_errores <- "/home/ricardo/Personal_RM/Libro/Doc_Links/errores_urls.csv"
write.csv(df_errores_urls, archivo_errores, row.names = FALSE)
cat("‚úÖ Archivo creado con las URLs que presentan errores en:\n", archivo_errores)
total_urls <- nrow(df_urls_expandido)
total_ok <- sum(df_urls_verificadas$status_code == 200, na.rm = TRUE)
total_error <- nrow(df_errores_urls)
### resumen
# üßæ Resumen del chequeo de URLs
total_urls <- nrow(df_urls_expandido)
total_ok <- sum(df_urls_verificadas$status_code == 200, na.rm = TRUE)
df_errores_urls <- df_urls_verificadas %>%
filter(is.na(status_code) | status_code != 200)
df_urls_verificadas <- df_urls_expandido %>%
mutate(status_code = map_int(urls, check_url_status))
df_urls_verificadas
df_urls_verificadas <- df_urls_expandido %>%
mutate(status_code = map_int(urls, check_url_status))
View(df_urls_verificadas)
df_errores_urls <- df_urls_verificadas %>%
filter(is.na(status_code) | status_code != 200)
archivo_errores <- "/home/ricardo/Personal_RM/Libro/Doc_Links/errores_urls.csv"
write.csv(df_errores_urls, archivo_errores, row.names = FALSE)
cat("‚úÖ Archivo creado con las URLs que presentan errores en:\n", archivo_errores)
### resumen
# üßæ Resumen del chequeo de URLs
total_urls <- nrow(df_urls_expandido)
total_ok <- sum(df_urls_verificadas$status_code == 200, na.rm = TRUE)
total_error <- nrow(df_errores_urls)
cat("üìä RESUMEN DE VERIFICACI√ìN:\n")
cat("üîπ Total de URLs verificadas:", total_urls, "\n")
cat("‚úÖ URLs correctas (200):", total_ok, "\n")
cat("üö® URLs con error:", total_error, "\n")
if (total_error == 0) {
cat("üéâ Todas las URLs parecen estar funcionando correctamente.\n")
} else {
cat("‚ö†Ô∏è Revisa el archivo de errores para m√°s detalles:\n", archivo_errores, "\n")
}
# Ruta del archivo Word
archivo_word <- "/home/ricardo/Personal_RM/Libro/Doc_Links/El_amor_se_paga_con_amor_agosto_9_2025.docx"
# Ruta del archivo Word
archivo_word <- "/home/ricardo/Personal_RM/Libro/Doc_Links/El_amor_se_paga_con_amor_agosto_9_2025.docx"
# Crea una carpeta temporal
carpeta_temp <- "temp_docx"
unzip(archivo_word, exdir = carpeta_temp)
## Leer todos los footnotes y contarlos
footnotes <- read_xml(file.path(carpeta_temp, "word/footnotes.xml"))
footnotes_texto_individual <- xml_find_all(footnotes, "//w:footnote") %>%
map_chr(xml_text)
df_footnotes <- tibble(
id = seq_along(footnotes_texto_individual),
texto = footnotes_texto_individual
)
archivo_footnotes <- "/home/ricardo/Personal_RM/Libro/Doc_Links/footnotes_completos.csv"
write.csv(df_footnotes, archivo_footnotes, row.names = FALSE)
cat("‚úÖ Archivo CSV creado con", nrow(df_footnotes), "footnotes en:\n", archivo_csv)
df_footnotes_urls <- df_footnotes %>%
mutate(urls = str_extract_all(texto, "https?://[^\\s]+"))
df_footnotes_urls <- df_footnotes %>%
mutate(urls = str_extract_all(texto, "https?://[^\\s]+"))
df_footnotes_urls <- df_footnotes %>%
mutate(urls = str_extract_all(texto, "https?://[^\\s]+")) %>%
mutate(urls = map_chr(urls, ~ if (length(.x) == 0) "" else paste(.x, collapse = ", ")))
archivo_footnotes_urls <- "/home/ricardo/Personal_RM/Libro/Doc_Links/footnotes_con_id_urls.csv"
write.csv(df_footnotes_urls, archivo_footnotes_urls, row.names = FALSE)
df_urls_expandido <- df_footnotes_urls %>%
# Filtrar solo los footnotes que tienen URLs
filter(urls != "") %>%
# Separar las URLs en filas individuales
separate_rows(urls, sep = ",\\s*") %>% ## funci√≥n de tidyr
# Limpiar espacios extra
mutate(urls = str_trim(urls))
#  Guardar el archivo expandido (opcional)
archivo_urls_expandido <- "/home/ricardo/Personal_RM/Libro/Doc_Links/footnotes_urls_expandido.csv"
write.csv(df_urls_expandido, archivo_urls_expandido, row.names = FALSE)
check_url_status <- function(url) {
tryCatch({
resp <- httr::HEAD(url, timeout(20)) # usamos HEAD porque es m√°s r√°pido que GET
status <- httr::status_code(resp)
return(status)
}, error = function(e) {
return(NA) # si hay error (timeout, DNS, etc.)
})
}
df_urls_verificadas <- df_urls_expandido %>%
mutate(status_code = map_int(urls, check_url_status))
quarto::quarto_render()
